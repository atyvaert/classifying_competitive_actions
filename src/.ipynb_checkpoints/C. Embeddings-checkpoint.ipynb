{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf43c7f3",
   "metadata": {},
   "source": [
    "# C. Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40e7241",
   "metadata": {},
   "source": [
    "In this notebook, I used word embeddings to transform my textual data into numerical represenations. For this purpose, I used the data was cleaned in the notebook 'B. Vectorization' and that can be found in the gold_data folder. Further, I use the same base classifiers and resampling techniques that I used in the previous notebook.\n",
    "\n",
    "First, I trained a Word2Vec model (both the continuous bag of words and the skipgram implementation) to transform my textual data. Then, I used Glove embeddings for text represenation. However, as I only have a limited dataset to train embedding models, I aslo used pretrained embedding models. Then, I could use these pretrained embeddings to transform my train and test set and train my models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5e5b6d",
   "metadata": {},
   "source": [
    "# 0. Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "705a75fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Packages #\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from scipy.stats import randint\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "# Sklearn Packages #\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, StratifiedKFold, cross_val_predict, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, make_scorer\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# NLTK Packages #\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob, Word\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Import necessary libraries for handling imbalanced data\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Embedding related imports\n",
    "import sys\n",
    "import gensim\n",
    "from gensim.models import Word2Vec, Doc2Vec\n",
    "from gensim.models.phrases import Phraser, Phrases\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b73f8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn of warnings, just to avoid pesky messages that might cause confusion here\n",
    "# Remove when testing your own code #\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d611c36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Headline</th>\n",
       "      <th>category</th>\n",
       "      <th>cleaned_headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>194578</td>\n",
       "      <td>Head Line: US Patent granted to BASF SE (Delaw...</td>\n",
       "      <td>None</td>\n",
       "      <td>head u patent granted se delaware may titled c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>564295</td>\n",
       "      <td>Societe Generale Launches a Next-Generation Ca...</td>\n",
       "      <td>None</td>\n",
       "      <td>societe generale launch nextgeneration card in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>504138</td>\n",
       "      <td>BARCLAYS PLC Form 8.3 - EUTELSAT COMMUNICATION...</td>\n",
       "      <td>None</td>\n",
       "      <td>plc form communication</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>91379</td>\n",
       "      <td>ASML: 4Q Earnings Snapshot</td>\n",
       "      <td>None</td>\n",
       "      <td>4q earnings snapshot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>265750</td>\n",
       "      <td>Form 8.3 - AXA INVESTMENT MANAGERS : Booker Gr...</td>\n",
       "      <td>None</td>\n",
       "      <td>form investment manager group plc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                           Headline category  \\\n",
       "0  194578  Head Line: US Patent granted to BASF SE (Delaw...     None   \n",
       "1  564295  Societe Generale Launches a Next-Generation Ca...     None   \n",
       "2  504138  BARCLAYS PLC Form 8.3 - EUTELSAT COMMUNICATION...     None   \n",
       "3   91379                         ASML: 4Q Earnings Snapshot     None   \n",
       "4  265750  Form 8.3 - AXA INVESTMENT MANAGERS : Booker Gr...     None   \n",
       "\n",
       "                                    cleaned_headline  \n",
       "0  head u patent granted se delaware may titled c...  \n",
       "1  societe generale launch nextgeneration card in...  \n",
       "2                             plc form communication  \n",
       "3                               4q earnings snapshot  \n",
       "4                  form investment manager group plc  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change to Working Directory with Training Data # \n",
    "#os.chdir(\"/Users/Artur/Desktop/thesis_HIR_versie5/coding\")\n",
    "os.chdir(\"/Users/juarel/Desktop/studies artur/thesis_HIR/coding\")\n",
    "\n",
    "# Load the preprocessed data #\n",
    "df_train = pd.read_csv(\"./data/gold_data/train.csv\", header = 0)\n",
    "df_test = pd.read_csv(\"./data/gold_data/test.csv\", header = 0)\n",
    "\n",
    "# inspect the data\n",
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb86cefd",
   "metadata": {},
   "source": [
    "# 1. Define functions and parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccab087",
   "metadata": {},
   "source": [
    "Before we continue, we first define some useful functions and parameters that we use throughout this notebook. The first four functions and parameters were also used and defined in the previous notebook.\n",
    "\n",
    "1. get_classification_metrics: Create a function that return the classification metrics for each model. The precision, recall and f1 score are all determined using the average value of all classes, without adjusting weights to these classes.\n",
    "\n",
    "2. Define a dataframe to store the results of the different models. Moreover, also define a dictionary that stores the best parameters for each model.\n",
    "\n",
    "3. Define the number of splits, the stratified cross validator to ensure class frequencies are considered, and the scoring metric based on the average F1 score. We use an F1 score as scoring metric as accuracy is not a good evaluation metric in our case.\n",
    "\n",
    "4. Define a function that trains the defined model, the input data, the classifier and its parameter grid. Besides, it will also take 4 parameters as input that give more information about the model that is being trained. This is usefull for the storage of the performance of the different algorithms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caab5f50",
   "metadata": {},
   "source": [
    "New functions specific to the embedding notebook:\n",
    "\n",
    "5. Create a function to create embeddings based on a trained model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faf61e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Function that returns classication metrics\n",
    "def get_classification_metrics(y_true, y_pred):\n",
    "    \n",
    "    # Calculate Model Performance Metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='macro')\n",
    "    recall = recall_score(y_true, y_pred, average='macro')\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "\n",
    "    return accuracy, precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4703149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create an empty dataframe to store the results of all the models\n",
    "results_all_df = pd.DataFrame()\n",
    "\n",
    "# Add columns for the metrics\n",
    "columns = ['vectorizer', 'FS', 'classifier', 'resampling','accuracy', 'precision', 'recall', 'f1']\n",
    "for col in columns:\n",
    "    results_all_df[col] = 0\n",
    "\n",
    "# create an empty dictionary to store the optimal parameters\n",
    "best_params_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d97903e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Define different parameters\n",
    "# Define the number of folds for cross-validation\n",
    "n_splits = 5\n",
    "\n",
    "# Initialize the stratified k-fold object\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42) # ensures class balances are kept\n",
    "\n",
    "# Define the scoring metric\n",
    "scoring = make_scorer(f1_score, average= 'macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab34db38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Define a function to train and evaluate the different models\n",
    "def perform_grid_search(name, model, param_grid, X_train, X_test, y_train, y_test,\n",
    "                       vectorizer, FS, classifier, resampling):\n",
    "    \n",
    "    # Define a seed value\n",
    "    random.seed(7)\n",
    "        \n",
    "    # Perform the grid search using cross-validation\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=skf, scoring=scoring)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best model and its hyperparameters\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    # Store the best parameters for the current category in the dictionary\n",
    "    best_params_dict[name] = best_params\n",
    "    print(f'best parameters: {best_params}')\n",
    "\n",
    "    # Retrain the best model with the whole training set\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    # Calculate the probabilities (not for SVM as this is not possible)\n",
    "    if classifier != 'SVM':\n",
    "        y_pred_proba = best_model.predict_proba(X_test)\n",
    "        \n",
    "        # Find the highest probability for each observation\n",
    "        highest_prob = np.amax(y_pred_proba, axis = 1)\n",
    "    \n",
    "        # Create a DataFrame with test observations, highest probabilities, and predicted classes\n",
    "        predictions_df = pd.DataFrame({'Observation_nr': y_test.index, 'Probability': highest_prob, 'Prediction': y_pred})\n",
    "        \n",
    "    else:\n",
    "        # Create a DataFrame with test observations and predicted classes\n",
    "        predictions_df = pd.DataFrame({'Observation_nr': y_test.index, 'Prediction': y_pred})\n",
    "        \n",
    "    # Store the final predictions with its probability for the test set\n",
    "    predictions_df.to_csv(f'./Output/predictions/{name}.csv', index = False, header = True)\n",
    "    #predictions_df.to_excel(f'./Output/predictions/{name}.xlsx', index = False, header = True)\n",
    "\n",
    "    # Calculate the classification metrics\n",
    "    accuracy, precision, recall, f1 = get_classification_metrics(y_test, y_pred)\n",
    "    \n",
    "    # print the results\n",
    "    print(f'Results for {name}:')\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'F1: {f1}')\n",
    "    \n",
    "    # add the results to the dataframe with all the results\n",
    "    results_all_df.loc[name] = [vectorizer, FS, classifier, resampling, accuracy, precision, recall, f1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86ffaa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Define a function to transform the data into embeddings based on a trained model\n",
    "def vectorize_embedding(headline, model, size):\n",
    "    \n",
    "    # Split the headline into individual words\n",
    "    words = headline.split()\n",
    "\n",
    "    # Retrieve word vectors for each word in the headline\n",
    "    words_vecs = [model.wv[word] for word in words if word in model.wv]\n",
    "\n",
    "    # If no word vectors are found (i.e., no matching words in the pretrained model),\n",
    "    # return a zero vector of the defined dimension\n",
    "    if len(words_vecs) == 0:\n",
    "        return np.zeros(size)\n",
    "\n",
    "    # Convert the list of word vectors into a numpy array and average across all words\n",
    "    words_vecs = np.array(words_vecs)\n",
    "    mean_vector = words_vecs.mean(axis=0)\n",
    "\n",
    "    return mean_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab6ad650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the independent and dependent variables\n",
    "X_train = df_train['cleaned_headline']\n",
    "X_test = df_test['cleaned_headline']\n",
    "\n",
    "y_train = df_train['category']\n",
    "y_test = df_test['category']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c12a88",
   "metadata": {},
   "source": [
    "# 2. Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9512868c",
   "metadata": {},
   "source": [
    "In this notebook, I used the same base classifiers and resampling techniques as in the previous notebook 'B. Vectorization'. Moreover, I also initialize the models the first time they are used (section 2.1). Further, I again hypertune the parameters of each model trough cross validation with GridSearchCV. Moroever, I used the same hypertuned the parameters using the same values as in the previous notebook.\n",
    "\n",
    "Further, I also used the same resampling strategies as in the previous notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15b018e",
   "metadata": {},
   "source": [
    "## 2.1 Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b899bf9",
   "metadata": {},
   "source": [
    "The first embedding model that I used to transfrom my textual data into numerical vectors. Word2Vec is a neural network-based technique designed by Google to learn vector embeddings. Besides, it has two different ways of learning its context, which will both be implemented in this notebook:\n",
    "\n",
    "1. CBOW = Continuous bag of words. This model trains each word against its context. In other words, what words are likely to appear near a given word.\n",
    "2. Skipgram = This model trains each context against the word. Here, the context is given and the algorithm searches for the word that is likely to appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dda4e3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define with what vectorizer we build the models with for storage\n",
    "vectorizer = 'Word2Vec'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc61da4",
   "metadata": {},
   "source": [
    "## 2.1.1 Create word embeddings with CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "607f4832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the implementation method of word2vec\n",
    "FS = 'cbow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f720150c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create an array of seperate words for each headline as input for the model\n",
    "headlines_train = [headline.split() for headline in X_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5adce57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dimension of the vector\n",
    "size = 300\n",
    "\n",
    "# define the cbow_model and train on training set headlines\n",
    "cbow_model = Word2Vec(headlines_train, \n",
    "                 min_count = 2,          # Ignore words that appear less than this\n",
    "                 vector_size = size,      # Dimensionality of word embeddings\n",
    "                 workers = 8,            # Number of processors (parallelisation)\n",
    "                 window = 5,             # Context window for words during training\n",
    "                 epochs = 50,            # Number of epochs training over corpus\n",
    "                 sg = 0)                 # 0 for CBOW and 1 for skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b72eec3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the independent variable of the train and the test set\n",
    "X_train_cbow = np.array([vectorize_embedding(headline, cbow_model, size) for headline in X_train])\n",
    "X_test_cbow = np.array([vectorize_embedding(headline, cbow_model, size) for headline in X_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bb63da",
   "metadata": {},
   "source": [
    "### 2.1.1.1 Without resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a7f0a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the resampling technique\n",
    "resampling = 'None'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c32853",
   "metadata": {},
   "source": [
    "#### A. Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "feb03e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model characteristics\n",
    "model_name = 'cbow_log_w'\n",
    "classifier = 'logR'\n",
    "\n",
    "# Initialize the classifier\n",
    "logreg = LogisticRegression(random_state = 7)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid_log = {\n",
    "    'penalty': ['None', 'l2'], # normal or ridge regression\n",
    "    'C': [0.1, 1, 10]          # The inverse penalization term (smaller is higher penalization)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0bc2428",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters: {'C': 10, 'penalty': 'l2'}\n",
      "Results for cbow_log_w:\n",
      "Accuracy: 0.9144310823311749\n",
      "Precision: 0.48050296715507107\n",
      "Recall: 0.32663128643473693\n",
      "F1: 0.3811327376422734\n"
     ]
    }
   ],
   "source": [
    "# perform a grid search for the logistic regression model\n",
    "# the results are automatically stored in results_all_df and best_params_dict\n",
    "perform_grid_search(model_name, logreg, param_grid_log, X_train_cbow, X_test_cbow, y_train, y_test,\n",
    "                   vectorizer, FS, classifier, resampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb18666",
   "metadata": {},
   "source": [
    "#### B. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b663d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model characteristics\n",
    "model_name = 'cbow_DT_w'\n",
    "classifier = 'DT'\n",
    "\n",
    "# Initialize the classifier\n",
    "tree = DecisionTreeClassifier(random_state = 7)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid_DT = {\n",
    "    'criterion': ['gini'],          # Define the splitting criteria: Gini index for node impurity\n",
    "    'min_samples_leaf': [1, 2],     # Define the minimum number of samples required to be at leaf node\n",
    "    'max_features': [None]  # Define the number of features to consider when looking for the best split\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e38d4198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters: {'criterion': 'gini', 'max_features': None, 'min_samples_leaf': 1}\n",
      "Results for cbow_DT_w:\n",
      "Accuracy: 0.8604995374653099\n",
      "Precision: 0.20657184235685688\n",
      "Recall: 0.21924204195660818\n",
      "F1: 0.21227693259619926\n"
     ]
    }
   ],
   "source": [
    "# perform a grid search for the logistic regression model\n",
    "# the results are automatically stored in results_all_df and best_params_dict\n",
    "perform_grid_search(model_name, tree, param_grid_DT, X_train_cbow, X_test_cbow, y_train, y_test,\n",
    "                   vectorizer, FS, classifier, resampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87582e6",
   "metadata": {},
   "source": [
    "#### C. Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba127b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model characteristics\n",
    "model_name = 'cbow_svm_w'\n",
    "classifier = 'SVM'\n",
    "\n",
    "# Initialize the classifier\n",
    "svm = SVC(random_state = 7)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid_svm = {\n",
    "    'C': [0.1, 1, 10, 100], # inverse regularization parameter\n",
    "    'kernel': ['linear', 'poly', 'rbf'], # what type of kernel need to be used (rbf = radial kernel)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47a7dca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters: {'C': 10, 'kernel': 'rbf'}\n",
      "Results for cbow_svm_w:\n",
      "Accuracy: 0.9227567067530065\n",
      "Precision: 0.5847363045740075\n",
      "Recall: 0.3731083007676317\n",
      "F1: 0.4431062347607094\n"
     ]
    }
   ],
   "source": [
    "# perform a grid search for the logistic regression model\n",
    "# the results are automatically stored in results_all_df and best_params_dict\n",
    "perform_grid_search(model_name, svm, param_grid_svm, X_train_cbow, X_test_cbow, y_train, y_test,\n",
    "                   vectorizer, FS, classifier, resampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d27ae5",
   "metadata": {},
   "source": [
    "#### D. Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa3955b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model characteristics\n",
    "model_name = 'cbow_rf_w'\n",
    "classifier = 'RF'\n",
    "\n",
    "# Initialize the classifier\n",
    "rfc = RandomForestClassifier(random_state = 7, n_jobs = -1)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid_rf = {\n",
    "    'criterion': ['gini'],          # Define the splitting criteria: Gini index for node impurity\n",
    "    'n_estimators': [100, 500],     # the number of trees to use when building the model\n",
    "    'min_samples_leaf': [1, 2],     # Define the minimum number of samples required to be at leaf node\n",
    "    'max_features': ['sqrt']        # Define the number of features to consider when looking for the best split\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "14eac49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters: {'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 1, 'n_estimators': 100}\n",
      "Results for cbow_rf_w:\n",
      "Accuracy: 0.9106382978723404\n",
      "Precision: 0.6163458582489317\n",
      "Recall: 0.1693731748647293\n",
      "F1: 0.226366175679055\n"
     ]
    }
   ],
   "source": [
    "# perform a grid search for the logistic regression model\n",
    "# the results are automatically stored in results_all_df and best_params_dict\n",
    "perform_grid_search(model_name, rfc, param_grid_rf, X_train_cbow, X_test_cbow, y_train, y_test,\n",
    "                   vectorizer, FS, classifier, resampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5bd69e",
   "metadata": {},
   "source": [
    "#### E. Adaboost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce9de3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model characteristics\n",
    "model_name = 'cbow_ada_w'\n",
    "classifier = 'ADA'\n",
    "\n",
    "# Initialize decision tree base estimator for AdaBoost\n",
    "base_estimator = DecisionTreeClassifier(random_state = 7)\n",
    "\n",
    "# Initialize AdaBoost classifier\n",
    "ada = AdaBoostClassifier(base_estimator = base_estimator, random_state = 7)\n",
    "\n",
    "# Define parameter grid for AdaBoost\n",
    "param_grid_ada = {\n",
    "    'n_estimators': [50, 100, 200],   # the maximum number of estimators before boosting is terminated\n",
    "    'learning_rate': [0.01, 0.1, 0.5], # weight applied to each classifier at boosting iteration\n",
    "                                      # A higher learning rate increases the contribution of each classifier. \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "96cf4aea",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-99308c96860e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# perform a grid search for the logistic regression model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# the results are automatically stored in results_all_df and best_params_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m perform_grid_search(model_name, ada, param_grid_ada, X_train_cbow, X_test_cbow, y_train, y_test,\n\u001b[0m\u001b[1;32m      4\u001b[0m                    vectorizer, FS, classifier, resampling)\n",
      "\u001b[0;32m<ipython-input-7-84f9eb7842a6>\u001b[0m in \u001b[0;36mperform_grid_search\u001b[0;34m(name, model, param_grid, X_train, X_test, y_train, y_test, vectorizer, FS, classifier, resampling)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Perform the grid search using cross-validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mgrid_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Get the best model and its hyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    872\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1386\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1388\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    819\u001b[0m                     )\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    822\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1086\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1088\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1089\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 901\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    902\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    595\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    289\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    289\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;31m# Boosting step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             sample_weight, estimator_weight, estimator_error = self._boost(\n\u001b[0m\u001b[1;32m    163\u001b[0m                 \u001b[0miboost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_weight_boosting.py\u001b[0m in \u001b[0;36m_boost\u001b[0;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[1;32m    567\u001b[0m         \"\"\"\n\u001b[1;32m    568\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithm\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"SAMME.R\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_boost_real\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miboost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# elif self.algorithm == \"SAMME\":\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_weight_boosting.py\u001b[0m in \u001b[0;36m_boost_real\u001b[0;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0my_predict_proba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    887\u001b[0m         \"\"\"\n\u001b[1;32m    888\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         super().fit(\n\u001b[0m\u001b[1;32m    890\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    377\u001b[0m             )\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# perform a grid search for the logistic regression model\n",
    "# the results are automatically stored in results_all_df and best_params_dict\n",
    "perform_grid_search(model_name, ada, param_grid_ada, X_train_cbow, X_test_cbow, y_train, y_test,\n",
    "                   vectorizer, FS, classifier, resampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff40b2e5",
   "metadata": {},
   "source": [
    "### 2.1.1.2 With undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efd195b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the resampling technique\n",
    "resampling = 'Und'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac4e41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the categories and the maximum number of samples\n",
    "categories = df_train[\"category\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ee76cc",
   "metadata": {},
   "source": [
    "#### Random undersampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af088ae",
   "metadata": {},
   "source": [
    "Again, we used the same undersampling strategy as we used in the notebook 'Vectorization'. We used random undersampling and reduced the imbalance ratio to 4:1 for each category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dc2794",
   "metadata": {},
   "source": [
    "#### Second strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f8b62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of samples in the biggest minority category\n",
    "rus_n = df_train['category'].value_counts().sort_values(ascending=False)[1]\n",
    "\n",
    "# Dictionary to store the actual maximum imbalance per class for undersampling\n",
    "max_imbalance_u = {}\n",
    "\n",
    "# Calculate the actual maximum imbalance for each class\n",
    "for category in categories:\n",
    "    if category == 'None':\n",
    "        max_imbalance_u[category] = rus_n\n",
    "    else:\n",
    "        # Set the actual maximum to the number of available samples\n",
    "        max_imbalance_u[category] = y_train.value_counts()[category]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c83283d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the random undersampler with maximum imbalance\n",
    "undersampler = RandomUnderSampler(sampling_strategy = max_imbalance_u, random_state = 7)\n",
    "\n",
    "# Undersample the data\n",
    "X_train_cbow_und, y_train_cbow_und = undersampler.fit_resample(X_train_cbow, y_train)\n",
    "#y_train_cbow_und.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd752072",
   "metadata": {},
   "source": [
    "#### A. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c39c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model characteristics\n",
    "model_name = 'cbow_log_u'\n",
    "classifier = 'logR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6f1ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform a grid search for the logistic regression model\n",
    "# the results are automatically stored in results_all_df and best_params_dict\n",
    "perform_grid_search(model_name, logreg, param_grid_log, X_train_cbow_und, X_test_cbow, y_train_cbow_und, y_test,\n",
    "                   vectorizer, FS, classifier, resampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77751dd0",
   "metadata": {},
   "source": [
    "#### B. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294863fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model characteristics\n",
    "model_name = 'cbow_DT_u'\n",
    "classifier = 'DT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c34879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform a grid search for the logistic regression model\n",
    "# the results are automatically stored in results_all_df and best_params_dict\n",
    "perform_grid_search(model_name, tree, param_grid_DT, X_train_cbow_und, X_test_cbow, y_train_cbow_und, y_test,\n",
    "                   vectorizer, FS, classifier, resampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb224122",
   "metadata": {},
   "source": [
    "#### C. Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50569b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model characteristics\n",
    "model_name = 'cbow_svm_u'\n",
    "classifier = 'SVM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5a7ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform a grid search for the logistic regression model\n",
    "# the results are automatically stored in results_all_df and best_params_dict\n",
    "perform_grid_search(model_name, svm, param_grid_svm, X_train_cbow_und, X_test_cbow, y_train_cbow_und, y_test,\n",
    "                   vectorizer, FS, classifier, resampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8cd8d6",
   "metadata": {},
   "source": [
    "#### D. Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada17123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model characteristics\n",
    "model_name = 'cbow_rf_u'\n",
    "classifier = 'RF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e835e960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform a grid search for the logistic regression model\n",
    "# the results are automatically stored in results_all_df and best_params_dict\n",
    "perform_grid_search(model_name, rfc, param_grid_rf, X_train_cbow_und, X_test_cbow, y_train_cbow_und, y_test,\n",
    "                   vectorizer, FS, classifier, resampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c76cc2",
   "metadata": {},
   "source": [
    "#### E. Adaboost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef66e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model characteristics\n",
    "model_name = 'cbow_ada_u'\n",
    "classifier = 'ADA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0214c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform a grid search for the logistic regression model\n",
    "# the results are automatically stored in results_all_df and best_params_dict\n",
    "perform_grid_search(model_name, ada, param_grid_ada, X_train_cbow_und, X_test_cbow, y_train_cbow_und, y_test,\n",
    "                   vectorizer, FS, classifier, resampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc769c64",
   "metadata": {},
   "source": [
    "### 2.1.1.3 With oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dafedce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the resampling technique\n",
    "resampling = 'Ove'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53378613",
   "metadata": {},
   "source": [
    "#### Oversampling with SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052c7aa0",
   "metadata": {},
   "source": [
    "Again, we used the same oversampling strategy as we used in the notebook 'Vectorization'. We used SMOTE and reduced the imbalance ratio to 4:1 for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab748192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of samples in the majority class\n",
    "ove_n = df_train['category'].value_counts().sort_values(ascending=False)[0]\n",
    "\n",
    "# Oversample until the number of observations equals a fourth of the majority class\n",
    "max_samples = int(ove_n/4)\n",
    "\n",
    "# Dictionary to store the actual maximum imbalance per class for oversampling\n",
    "max_imbalance_o = {}\n",
    "\n",
    "# Calculate the actual maximum imbalance for each class\n",
    "for category in categories:\n",
    "    if category == 'None':\n",
    "        max_imbalance_o[category] = y_train.value_counts()[category]\n",
    "    else:\n",
    "        # Set the actual maximum to the number of available samples\n",
    "        max_imbalance_o[category] = max_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c54e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the SMOTE oversampler\n",
    "oversampler = SMOTE(sampling_strategy=max_imbalance_o, random_state=7)\n",
    "\n",
    "# Undersample the data\n",
    "X_train_cbow_ove, y_train_cbow_ove = oversampler.fit_resample(X_train_cbow, y_train)\n",
    "y_train_cbow_ove.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca3e0aa",
   "metadata": {},
   "source": [
    "#### A. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e731af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model characteristics\n",
    "model_name = 'cbow_log_o'\n",
    "classifier = 'logR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d307746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform a grid search for the logistic regression model\n",
    "# the results are automatically stored in results_all_df and best_params_dict\n",
    "perform_grid_search(model_name, logreg, param_grid_log, X_train_cbow_ove, X_test_cbow, y_train_cbow_ove, y_test,\n",
    "                   vectorizer, FS, classifier, resampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b41767",
   "metadata": {},
   "source": [
    "#### B. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08c63da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model characteristics\n",
    "model_name = 'cbow_DT_o'\n",
    "classifier = 'DT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42368b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform a grid search for the logistic regression model\n",
    "# the results are automatically stored in results_all_df and best_params_dict\n",
    "perform_grid_search(model_name, tree, param_grid_DT, X_train_cbow_ove, X_test_cbow, y_train_cbow_ove, y_test,\n",
    "                   vectorizer, FS, classifier, resampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bb2138",
   "metadata": {},
   "source": [
    "#### C. Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9fadbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model characteristics\n",
    "model_name = 'cbow_svm_o'\n",
    "classifier = 'SVM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46798c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform a grid search for the logistic regression model\n",
    "# the results are automatically stored in results_all_df and best_params_dict\n",
    "perform_grid_search(model_name, svm, param_grid_svm, X_train_cbow_ove, X_test_cbow, y_train_cbow_ove, y_test,\n",
    "                   vectorizer, FS, classifier, resampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e739f4",
   "metadata": {},
   "source": [
    "#### D. Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe5a2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model characteristics\n",
    "model_name = 'cbow_rf_o'\n",
    "classifier = 'RF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df561e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform a grid search for the logistic regression model\n",
    "# the results are automatically stored in results_all_df and best_params_dict\n",
    "perform_grid_search(model_name, rfc, param_grid_rf, X_train_cbow_ove, X_test_cbow, y_train_cbow_ove, y_test,\n",
    "                   vectorizer, FS, classifier, resampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d1015a",
   "metadata": {},
   "source": [
    "#### E. Adaboost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba687ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model characteristics\n",
    "model_name = 'cbow_ada_o'\n",
    "classifier = 'ADA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78eb5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform a grid search for the logistic regression model\n",
    "# the results are automatically stored in results_all_df and best_params_dict\n",
    "perform_grid_search(model_name, ada, param_grid_ada, X_train_cbow_ove, X_test_cbow, y_train_cbow_ove, y_test,\n",
    "                   vectorizer, FS, classifier, resampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbddde1",
   "metadata": {},
   "source": [
    "## 2.1.2 Create word embeddings with Skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e366467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the implementation method of word2vec\n",
    "FS = 'skip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b4f94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dimension of the vector\n",
    "size = 300\n",
    "\n",
    "# define the cbow_model and train on training set headlines\n",
    "skip_model = Word2Vec(headlines_train, \n",
    "                 min_count = 2,          # Ignore words that appear less than this\n",
    "                 vector_size = size,      # Dimensionality of word embeddings\n",
    "                 workers = 8,            # Number of processors (parallelisation)\n",
    "                 window = 5,             # Context window for words during training\n",
    "                 epochs = 20,            # Number of epochs training over corpus\n",
    "                 sg = 1)                 # 0 for CBOW and 1 for skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a803622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the independent variable of the train and the test set\n",
    "X_train_skip = np.array([vectorize_embedding(headline, skip_model, size) for headline in X_train])\n",
    "X_test_skip = np.array([vectorize_embedding(headline, skip_model, size) for headline in X_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82157e11",
   "metadata": {},
   "source": [
    "### 2.1.2.1 Without resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38757f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the resampling technique\n",
    "resampling = 'None'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2377a5df",
   "metadata": {},
   "source": [
    "#### A. Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5a57ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model characteristics\n",
    "model_name = 'skip_log_w'\n",
    "classifier = 'logR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db34ca7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# perform a grid search for the logistic regression model\n",
    "# the results are automatically stored in results_all_df and best_params_dict\n",
    "perform_grid_search(model_name, logreg, param_grid_log, X_train_skip, X_test_skip, y_train, y_test,\n",
    "                   vectorizer, FS, classifier, resampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52b1fa5",
   "metadata": {},
   "source": [
    "#### B. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83f7888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model characteristics\n",
    "model_name = 'skip_DT_w'\n",
    "classifier = 'DT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd267db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform a grid search for the logistic regression model\n",
    "# the results are automatically stored in results_all_df and best_params_dict\n",
    "perform_grid_search(model_name, tree, param_grid_DT, X_train_skip, X_test_skip, y_train, y_test,\n",
    "                   vectorizer, FS, classifier, resampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0584b692",
   "metadata": {},
   "source": [
    "#### C. Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ef1b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model characteristics\n",
    "model_name = 'skip_svm_w'\n",
    "classifier = 'SVM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8add2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform a grid search for the logistic regression model\n",
    "# the results are automatically stored in results_all_df and best_params_dict\n",
    "perform_grid_search(model_name, svm, param_grid_svm, X_train_skip, X_test_skip, y_train, y_test,\n",
    "                   vectorizer, FS, classifier, resampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4882f859",
   "metadata": {},
   "source": [
    "#### D. Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8f0a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model characteristics\n",
    "model_name = 'skip_rf_w'\n",
    "classifier = 'RF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adac73ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform a grid search for the logistic regression model\n",
    "# the results are automatically stored in results_all_df and best_params_dict\n",
    "perform_grid_search(model_name, rfc, param_grid_rf, X_train_skip, X_test_skip, y_train, y_test,\n",
    "                   vectorizer, FS, classifier, resampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcafa60",
   "metadata": {},
   "source": [
    "#### E. Adaboost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d605285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model characteristics\n",
    "model_name = 'skip_ada_w'\n",
    "classifier = 'ADA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59028886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform a grid search for the logistic regression model\n",
    "# the results are automatically stored in results_all_df and best_params_dict\n",
    "perform_grid_search(model_name, ada, param_grid_ada, X_train_skip, X_test_skip, y_train, y_test,\n",
    "                   vectorizer, FS, classifier, resampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d4109d",
   "metadata": {},
   "source": [
    "### 2.1.2.2 With undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d6f0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the resampling technique\n",
    "resampling = 'Und'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def53ffe",
   "metadata": {},
   "source": [
    "#### Second strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abace06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undersample the data\n",
    "X_train_skip_und, y_train_skip_und = undersampler.fit_resample(X_train_skip, y_train)\n",
    "#y_train_cbow_und.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e6438b",
   "metadata": {},
   "source": [
    "#### A. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1923cb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model characteristics\n",
    "model_name = 'skip_log_u'\n",
    "classifier = 'logR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb5cbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform a grid search for the logistic regression model\n",
    "# the results are automatically stored in results_all_df and best_params_dict\n",
    "perform_grid_search(model_name, logreg, param_grid_log, X_train_skip_und, X_test_skip, y_train_skip_und, y_test,\n",
    "                   vectorizer, FS, classifier, resampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98238e53",
   "metadata": {},
   "source": [
    "#### B. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c9e57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model characteristics\n",
    "model_name = 'skip_DT_u'\n",
    "classifier = 'DT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7124e0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform a grid search for the logistic regression model\n",
    "# the results are automatically stored in results_all_df and best_params_dict\n",
    "perform_grid_search(model_name, tree, param_grid_DT, X_train_skip_und, X_test_skip, y_train_skip_und, y_test,\n",
    "                   vectorizer, FS, classifier, resampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1189ac57",
   "metadata": {},
   "source": [
    "#### C. Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ff3686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model characteristics\n",
    "model_name = 'skip_svm_u'\n",
    "classifier = 'SVM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244d5f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform a grid search for the logistic regression model\n",
    "# the results are automatically stored in results_all_df and best_params_dict\n",
    "perform_grid_search(model_name, svm, param_grid_svm, X_train_skip_und, X_test_skip, y_train_skip_und, y_test,\n",
    "                   vectorizer, FS, classifier, resampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c661bede",
   "metadata": {},
   "source": [
    "#### D. Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5178cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model characteristics\n",
    "model_name = 'skip_rf_u'\n",
    "classifier = 'RF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e342634b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform a grid search for the logistic regression model\n",
    "# the results are automatically stored in results_all_df and best_params_dict\n",
    "perform_grid_search(model_name, rfc, param_grid_rf, X_train_skip_und, X_test_skip, y_train_skip_und, y_test,\n",
    "                   vectorizer, FS, classifier, resampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72558681",
   "metadata": {},
   "source": [
    "#### E. Adaboost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af83e1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model characteristics\n",
    "model_name = 'skip_ada_u'\n",
    "classifier = 'ADA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1c1ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform a grid search for the logistic regression model\n",
    "# the results are automatically stored in results_all_df and best_params_dict\n",
    "perform_grid_search(model_name, ada, param_grid_ada, X_train_skip_und, X_test_skip, y_train_skip_und, y_test,\n",
    "                   vectorizer, FS, classifier, resampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86df289",
   "metadata": {},
   "source": [
    "### 2.1.2.3 With oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3b5d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the resampling technique\n",
    "resampling = 'Ove'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3d74b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undersample the data\n",
    "X_train_skip_ove, y_train_skip_ove = oversampler.fit_resample(X_train_skip, y_train)\n",
    "#y_train_skip_ove.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35594c82",
   "metadata": {},
   "source": [
    "#### A. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe347cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model characteristics\n",
    "model_name = 'skip_log_o'\n",
    "classifier = 'logR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96249416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform a grid search for the logistic regression model\n",
    "# the results are automatically stored in results_all_df and best_params_dict\n",
    "perform_grid_search(model_name, logreg, param_grid_log, X_train_skip_ove, X_test_skip, y_train_skip_ove, y_test,\n",
    "                   vectorizer, FS, classifier, resampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dff4d1f",
   "metadata": {},
   "source": [
    "#### B. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b156aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model characteristics\n",
    "model_name = 'skip_DT_o'\n",
    "classifier = 'DT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35cbd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform a grid search for the logistic regression model\n",
    "# the results are automatically stored in results_all_df and best_params_dict\n",
    "perform_grid_search(model_name, tree, param_grid_DT, X_train_skip_ove, X_test_skip, y_train_skip_ove, y_test,\n",
    "                   vectorizer, FS, classifier, resampling)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d98f52",
   "metadata": {},
   "source": [
    "#### C. Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392ebbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model characteristics\n",
    "model_name = 'skip_svm_o'\n",
    "classifier = 'SVM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc27e74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform a grid search for the logistic regression model\n",
    "# the results are automatically stored in results_all_df and best_params_dict\n",
    "perform_grid_search(model_name, svm, param_grid_svm, X_train_skip_ove, X_test_skip, y_train_skip_ove, y_test,\n",
    "                   vectorizer, FS, classifier, resampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b51615",
   "metadata": {},
   "source": [
    "#### D. Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6216b3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model characteristics\n",
    "model_name = 'skip_rf_o'\n",
    "classifier = 'RF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f6b3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform a grid search for the logistic regression model\n",
    "# the results are automatically stored in results_all_df and best_params_dict\n",
    "perform_grid_search(model_name, rfc, param_grid_rf, X_train_skip_ove, X_test_skip, y_train_skip_ove, y_test,\n",
    "                   vectorizer, FS, classifier, resampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ddb647",
   "metadata": {},
   "source": [
    "#### E. Adaboost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fdb244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model characteristics\n",
    "model_name = 'skip_ada_o'\n",
    "classifier = 'ADA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f16679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform a grid search for the logistic regression model\n",
    "# the results are automatically stored in results_all_df and best_params_dict\n",
    "perform_grid_search(model_name, ada, param_grid_ada, X_train_skip_ove, X_test_skip, y_train_skip_ove, y_test,\n",
    "                   vectorizer, FS, classifier, resampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1494d2dc",
   "metadata": {},
   "source": [
    "#### Intermediate: write the data away"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a07cbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write away results\n",
    "results_all_df.to_csv('./Output/Model performance/results_embeddings.csv', index = False, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9132e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the dictionary with the best parameters away\n",
    "with open('./Output/parameters/embeddings.json', 'w') as file:\n",
    "    json.dump(best_params_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c038f859",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
