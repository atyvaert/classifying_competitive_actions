{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1032b24c",
   "metadata": {},
   "source": [
    "# 1. Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceec6f4b",
   "metadata": {},
   "source": [
    "## A. Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b161f7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Packages #\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Load TQDM to Show Progress Bars #\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm as tqdm_notebook\n",
    "\n",
    "# Sklearn Packages #\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, StratifiedKFold, cross_val_predict, RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import Lasso, LassoCV, SGDClassifier, LogisticRegression, RidgeCV, RidgeClassifierCV, HuberRegressor, LinearRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# NLTK Packages #\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob, Word\n",
    "\n",
    "# Import necessary libraries for handling imbalanced data\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "from imblearn.ensemble import RUSBoostClassifier\n",
    "from imblearn.ensemble import smote\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "428f62e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn off warnings, just to avoid pesky messages that might cause confusion here\n",
    "# Remove when testing your own code #\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c1cc0b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Headline</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>194578</td>\n",
       "      <td>Head Line: US Patent granted to BASF SE (Delaw...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>564295</td>\n",
       "      <td>Societe Generale Launches a Next-Generation Ca...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>504138</td>\n",
       "      <td>BARCLAYS PLC Form 8.3 - EUTELSAT COMMUNICATION...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>91379</td>\n",
       "      <td>ASML: 4Q Earnings Snapshot</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>265750</td>\n",
       "      <td>Form 8.3 - AXA INVESTMENT MANAGERS : Booker Gr...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                           Headline category\n",
       "0  194578  Head Line: US Patent granted to BASF SE (Delaw...     None\n",
       "1  564295  Societe Generale Launches a Next-Generation Ca...     None\n",
       "2  504138  BARCLAYS PLC Form 8.3 - EUTELSAT COMMUNICATION...     None\n",
       "3   91379                         ASML: 4Q Earnings Snapshot     None\n",
       "4  265750  Form 8.3 - AXA INVESTMENT MANAGERS : Booker Gr...     None"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change to Working Directory with Training Data # \n",
    "os.chdir(\"/Users/Artur/Desktop/thesis_HIR_versie5/coding\")\n",
    "\n",
    "# Load Training Data #\n",
    "df_train = pd.read_csv(\"./data/train_adjusted.csv\", header = 0)\n",
    "\n",
    "# inspect the data\n",
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537d27d2",
   "metadata": {},
   "source": [
    "## B. Handling class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4002e608",
   "metadata": {},
   "source": [
    "Here, I would handle the class imbalance issue in my dataset. I would perform this step before tuning any hyperparameters. I would solve the class imbalance in my dataset by using SMOTE and the Tomek links. This algorithm generates synthetic examples of the minority class. On the other hand, Tomek links are used to remove examples that are near the decision boundary between two classes. Together, they oversample the minority classes and undersample the majority class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e527ad5f",
   "metadata": {},
   "source": [
    "## C. Define the vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd902340",
   "metadata": {},
   "source": [
    "Define a function to clean and tokenize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38f4a5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Tokenizer\n",
    "def textblob_tokenizer(str_input):\n",
    "    \n",
    "    # Convert list to string\n",
    "    input_str = str_input\n",
    "    \n",
    "    if isinstance(input_str, list):\n",
    "        input_str = ' '.join(input_str)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    str_input = str_input.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Define stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Tokenize and lemmatize text\n",
    "    blob = TextBlob(str_input.lower())\n",
    "    tokens = [Word(token).lemmatize() for token in blob.words]\n",
    "\n",
    "    # Remove numbers and stop words\n",
    "    words = [token for token in tokens if not re.match('^\\d+$', token) and token not in stop_words]\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "577cb2cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_headline</th>\n",
       "      <th>Headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[head, line, u, patent, granted, basf, se, del...</td>\n",
       "      <td>Head Line: US Patent granted to BASF SE (Delaw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[societe, generale, launch, nextgeneration, ca...</td>\n",
       "      <td>Societe Generale Launches a Next-Generation Ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[barclays, plc, form, eutelsat, communication]</td>\n",
       "      <td>BARCLAYS PLC Form 8.3 - EUTELSAT COMMUNICATION...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[asml, 4q, earnings, snapshot]</td>\n",
       "      <td>ASML: 4Q Earnings Snapshot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[form, axa, investment, manager, booker, group...</td>\n",
       "      <td>Form 8.3 - AXA INVESTMENT MANAGERS : Booker Gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43249</th>\n",
       "      <td>[tomra, system, asa, tom, purchase, share]</td>\n",
       "      <td>Tomra Systems ASA: TOM: Purchase of own shares</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43250</th>\n",
       "      <td>[swiss, federal, institute, intellectual, prop...</td>\n",
       "      <td>Swiss Federal Institute of Intellectual Proper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43251</th>\n",
       "      <td>[icon, pfizer, roche, join, addplan, df, conso...</td>\n",
       "      <td>ICON: Pfizer and Roche Join ADDPLAN DF Consort...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43252</th>\n",
       "      <td>[rio, tinto, plc, transaction, share]</td>\n",
       "      <td>Rio Tinto PLC Transaction in Own Shares -3-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43253</th>\n",
       "      <td>[roche, building, capacity, front, next, gener...</td>\n",
       "      <td>Roche Building Capacity To Front Next Generati...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43254 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        cleaned_headline  \\\n",
       "0      [head, line, u, patent, granted, basf, se, del...   \n",
       "1      [societe, generale, launch, nextgeneration, ca...   \n",
       "2         [barclays, plc, form, eutelsat, communication]   \n",
       "3                         [asml, 4q, earnings, snapshot]   \n",
       "4      [form, axa, investment, manager, booker, group...   \n",
       "...                                                  ...   \n",
       "43249         [tomra, system, asa, tom, purchase, share]   \n",
       "43250  [swiss, federal, institute, intellectual, prop...   \n",
       "43251  [icon, pfizer, roche, join, addplan, df, conso...   \n",
       "43252              [rio, tinto, plc, transaction, share]   \n",
       "43253  [roche, building, capacity, front, next, gener...   \n",
       "\n",
       "                                                Headline  \n",
       "0      Head Line: US Patent granted to BASF SE (Delaw...  \n",
       "1      Societe Generale Launches a Next-Generation Ca...  \n",
       "2      BARCLAYS PLC Form 8.3 - EUTELSAT COMMUNICATION...  \n",
       "3                             ASML: 4Q Earnings Snapshot  \n",
       "4      Form 8.3 - AXA INVESTMENT MANAGERS : Booker Gr...  \n",
       "...                                                  ...  \n",
       "43249     Tomra Systems ASA: TOM: Purchase of own shares  \n",
       "43250  Swiss Federal Institute of Intellectual Proper...  \n",
       "43251  ICON: Pfizer and Roche Join ADDPLAN DF Consort...  \n",
       "43252        Rio Tinto PLC Transaction in Own Shares -3-  \n",
       "43253  Roche Building Capacity To Front Next Generati...  \n",
       "\n",
       "[43254 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect the cleaned data\n",
    "df_train['cleaned_headline'] = df_train['Headline'].apply(textblob_tokenizer)\n",
    "\n",
    "# check the data\n",
    "df_train[[\"cleaned_headline\", \"Headline\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010a7b3e",
   "metadata": {},
   "source": [
    "Defining model parameters to use in a grid search:\n",
    "1. MINDF = Minimum Document Frequency -- Minimum number of times a word needs to occur to be considered\n",
    "2. MAXDF = Maximum Document Frequency -- Maximum share of documents where a word needs to occur to be considered\n",
    "3. MF = Maximum number of features we would want to consider -- ranked by most frequently occurring\n",
    "4. NGrams -- Number of Word Pairs. Takes the form (Min, Max). E.g. (1, 2) means single words and word pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c0e06af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameter search space for the TfidfVectorizer\n",
    "tfidf_param_dist = {'max_features': randint(500, 2000),\n",
    "                    'max_df': [0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "                    'min_df': [1, 2, 5, 10],\n",
    "                    'ngram_range': [(1,1), (1,2), (1,3)]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc713a0d",
   "metadata": {},
   "source": [
    "Define the vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06c7ddb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TfidfVectorizer with default parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=textblob_tokenizer)\n",
    "\n",
    "# Convert the text data to a matrix of TF-IDF features\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df_train['Headline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d2f21e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "567378cf",
   "metadata": {},
   "source": [
    "#### Hyper tune the vectorizer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4254036f",
   "metadata": {},
   "source": [
    "Now, we still need to hypertune the vectorizer. Therefore, I first define a random forest classifier with default parameters. This rf classifier is hypertuned by cross validating its performance in terms of the G_mean score, while using the default parameters of the vectorizer.\n",
    "\n",
    "After I tuned the rf classifier, I use this classifier to hypertune the vectorizer by cross validating its performance in terms of the G_mean score. Then, these parameters will be used to train the other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52eddbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Random Forest classifier with default parameters\n",
    "rfc = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "032ceb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom scoring function\n",
    "def custom_scoring(estimator, X, y):\n",
    "    # Define a RandomForestClassifier with the same parameters as the estimator\n",
    "    rfc = RandomForestClassifier(**estimator.get_params())\n",
    "    \n",
    "    # Use cross_val_score to calculate the mean accuracy of the classifier\n",
    "    scores = cross_val_score(rfc, X, y, cv=5, scoring='G_mean')\n",
    "    \n",
    "    # Return the mean accuracy\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f85cd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameter search space for the Random Forest classifier\n",
    "rfc_param_dist = {'n_estimators': randint(50, 100),\n",
    "                  'max_features': randint(1, 10),\n",
    "                  'max_depth': randint(1, 10)}\n",
    "\n",
    "# Create a randomized search object with cross-validation\n",
    "random_search = RandomizedSearchCV(estimator=rfc,\n",
    "                                   param_distributions=rfc_param_dist,\n",
    "                                   n_iter=10,\n",
    "                                   cv=5,\n",
    "                                   scoring=custom_scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aea97dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for Random Forest:  {'max_depth': 5, 'max_features': 2, 'n_estimators': 84}\n"
     ]
    }
   ],
   "source": [
    "# Fit the randomized search object to the training data\n",
    "random_search.fit(X_tfidf, df_train['category'])\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print('Best hyperparameters for Random Forest: ', random_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b415e62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for TfidfVectorizer:  {'max_df': 0.9, 'max_features': 1347, 'min_df': 10, 'ngram_range': (1, 3)}\n"
     ]
    }
   ],
   "source": [
    "# Use the best hyperparameters for Random Forest to fit a model on the TF-IDF matrix\n",
    "best_rfc = RandomForestClassifier(**random_search.best_params_)\n",
    "best_rfc.fit(X_tfidf, df_train['category'])\n",
    "\n",
    "# Use the best hyperparameters for TfidfVectorizer to fit a model on the training data\n",
    "random_search = RandomizedSearchCV(estimator=tfidf_vectorizer,\n",
    "                                   param_distributions=tfidf_param_dist,\n",
    "                                   n_iter=10,\n",
    "                                   cv=5,\n",
    "                                   scoring=custom_scoring)\n",
    "\n",
    "# Fit the randomized search object to the training data\n",
    "random_search.fit(df_train['Headline'], df_train['category'])\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print('Best hyperparameters for TfidfVectorizer: ', random_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d9ef55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd92495",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f6004d1",
   "metadata": {},
   "source": [
    "## D. Hyper parameting tuning of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27718002",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c320c25c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1999504a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "765037d7",
   "metadata": {},
   "source": [
    "## E. Evaluating the different models with the optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d7c8562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store Data in Lists for Text Classification #\n",
    "IDs = np.array(df_train['id'].values.tolist())\n",
    "headlines = np.array(df_train['Headline'].values.tolist())\n",
    "Classes = df_train['category'].values.tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ce94371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform vectorization and extract feature names #\n",
    "Abstract_Vectors = vec.fit_transform(headlines)\n",
    "FEATURENAMES = vec.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717b388a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83f1a59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to calculate classification metrics\n",
    "\n",
    "def get_classification_metrics(y_true, y_pred, y_pred_proba):\n",
    "    # Calculate Model Performance Metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='macro')\n",
    "    recall = recall_score(y_true, y_pred, average='macro')\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Calculate the extended G-mean\n",
    "    g_mean = geometric_mean_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "    # Calculate the MAUC score\n",
    "    mauc = np.round(roc_auc_score(y_true, y_pred_proba, multi_class='ovo', average='macro'), 3)\n",
    "\n",
    "    return accuracy, precision, recall, f1, cm, g_mean, mauc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2778e3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Set of Classifiers and their parameters\n",
    "CLASSIFIERS = [\n",
    "               [\"RUSBoost\", RUSBoostClassifier(random_state=1)],\n",
    "               [\"SMOTEBoost (AdaBoost)\", make_pipeline(SMOTE(random_state=1), AdaBoostClassifier(random_state=1))],\n",
    "               [\"SMOTEBoost (Gradient Boosting)\", make_pipeline(SMOTE(random_state=1), GradientBoostingClassifier(random_state=1))],\n",
    "               [\"Random Forest\", RandomForestClassifier(n_estimators=100)],\n",
    "              ]\n",
    "\n",
    "# Number of Folds (Splits) for Cross Validation #\n",
    "NUM_OF_SPLITS = 5\n",
    "\n",
    "# Define whether you want to manually reweight the sample by oversampling the smaller class \n",
    "Reweight = False\n",
    "\n",
    "# Define arrays in which to store classification outputs # \n",
    "RESULTS = []\n",
    "Classified_Values =[]\n",
    "Classified_Values_p =[]\n",
    "\n",
    "\n",
    "# Loop Through Different Classifiers #\n",
    "for CL in tqdm_notebook(CLASSIFIERS, desc = \"Evaluating Classifiers\"):\n",
    "\n",
    "    # Extract Classifier Names & Model #\n",
    "    name  = CL[0]\n",
    "    Model = CL[1]\n",
    "\n",
    "    # Define Arrays to store Actual, Predicted and Ids variables (Because we are shuffling them in next step) # \n",
    "    y_actual = []\n",
    "    y_predicted = []\n",
    "    y_predicted_proba = []\n",
    "\n",
    "    id_s = []\n",
    "\n",
    "    # Loop through K Folds and Repeat Cross Validation #\n",
    "    KFoldSplitter = StratifiedKFold(n_splits=NUM_OF_SPLITS, shuffle=True, random_state=1)\n",
    "    \n",
    "    for train_i, test_i in tqdm_notebook(KFoldSplitter.split(Abstract_Vectors, Classes), \n",
    "                                         desc = 'Cross-Validating',\n",
    "                                         leave = False,\n",
    "                                         total = NUM_OF_SPLITS):\n",
    "\n",
    "        # Select Rows in Data Based on Indexes [train_i, test_i]\n",
    "        Y = np.asarray(Classes)\n",
    "\n",
    "        train_X, test_X = Abstract_Vectors[train_i], Abstract_Vectors[test_i]\n",
    "        train_y, test_y = Y[train_i], Y[test_i]\n",
    "        Train_IDs, Test_IDs = IDs[train_i], IDs[test_i]\n",
    "\n",
    "        # solving class imbalance issues      \n",
    "        temp_y = list(train_y)\n",
    "        temp_X = train_X.todense().tolist()\n",
    "\n",
    "        if Reweight == True:\n",
    "            \n",
    "            # Use SMOTE and Tomek links to handle class imbalance\n",
    "            smt = SMOTETomek(random_state=1)\n",
    "            X_train_balanced, y_train_balanced = smt.fit_resample(train_X, train_y)\n",
    "            \n",
    "        else:\n",
    "            X_train_balanced, y_train_balanced = train_X, train_y\n",
    "\n",
    "\n",
    "        # Train Model #\n",
    "        Results = Model.fit(X_train_balanced, y_train_balanced)\n",
    "        \n",
    "        # Perform Prediction on Holdout Sample # \n",
    "        y_pred = Model.predict(test_X.toarray())\n",
    "        y_pred_proba = Model.predict_proba(test_X.toarray())\n",
    "\n",
    "        # Add to List with Final Results # \n",
    "        y_actual = y_actual + list(test_y)\n",
    "        y_predicted = y_predicted + list(y_pred)\n",
    "        y_predicted_proba = y_predicted_proba + list(y_pred_proba)\n",
    "        id_s = id_s + list(Test_IDs)\n",
    "\n",
    "    # ---------------------------------------------------------- #\n",
    "    # This runs only after all of the folds have been classified # \n",
    "    # ---------------------------------------------------------- #\n",
    "\n",
    "    # Calculate classification metrics\n",
    "    Accuracy, Precision, Recall, F1, CM, G_mean, MAUC = get_classification_metrics(y_actual, y_predicted, y_predicted_proba)\n",
    "\n",
    "\n",
    "    # Round to 3 Decimal Places # \n",
    "    #FN = np.round(CM[0][0]/CM[0].sum(), 3)\n",
    "    #FP = np.round(CM[0][1]/CM[0].sum(), 3)\n",
    "    #TN = np.round(CM[1][0]/CM[1].sum(), 3)\n",
    "    #TP = np.round(CM[1][1]/CM[1].sum(), 3)\n",
    "\n",
    "    FN = np.round(CM[0][0]/(CM[0][0] + CM[1][0]), 3)\n",
    "    FP = np.round(CM[0][1]/(CM[0][1] + CM[1][1]), 3)\n",
    "    TN = np.round(CM[1][0]/(CM[0][0] + CM[1][0]), 3)\n",
    "    TP = np.round(CM[1][1]/(CM[0][1] + CM[1][1]), 3)\n",
    "\n",
    "\n",
    "    FN = np.round(CM[0][0]/(CM[0][0] + CM[1][0]), 3)\n",
    "    FP = np.round(CM[0][1]/(CM[0][1] + CM[1][1]), 3)\n",
    "    TN = np.round(CM[1][0]/(CM[0][0] + CM[1][0]), 3)\n",
    "    TP = np.round(CM[1][1]/(CM[0][1] + CM[1][1]), 3)\n",
    "\n",
    "    # Add Classification Performance Metrics to List #\n",
    "    RESULTS.append([name, TP, FN, FP, TN, \n",
    "                          np.round(Accuracy, 3),\n",
    "                          np.round(Precision, 3),\n",
    "                          np.round(Recall, 3),\n",
    "                          np.round(F1, 3),\n",
    "                          G_mean,\n",
    "                          MAUC])\n",
    "\n",
    "    # Add Classification Results to List # \n",
    "    Classified_Values.append(list(zip(len(id_s)*[name],id_s, y_actual, y_predicted, \n",
    "                                       len(id_s)*[G_mean], len(id_s)*[MAUC])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8f34cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert List of Model Performance Metrics to Dataframe #\n",
    "RESULTS_TABLE = pd.DataFrame(RESULTS, columns = [\"Name\", \"True-Positives\", \n",
    "                                                 \"False-Negatives\", \"False-Positives\", \n",
    "                                                 \"True-Negatives\",\"Accuracy\", \n",
    "                                                 \"Precision\", \"Recall\", \"F1\", \"G_mean\", \"MAUC\"] )\n",
    "RESULTS_TABLE[\"Type\"] = \"Bag of Words\"\n",
    "RESULTS_TABLE = RESULTS_TABLE[[\"Name\", \"True-Positives\", \n",
    "                                                 \"False-Negatives\", \"False-Positives\", \n",
    "                                                 \"True-Negatives\",\"Accuracy\", \n",
    "                                                 \"Precision\", \"Recall\", \"F1\", \"G_mean\", \"MAUC\"]]\n",
    "\n",
    "# Output Results #\n",
    "RESULTS_TABLE.sort_values(\"Accuracy\", ascending = False ).to_csv(\"./Output/Model Performance/BOW Model Classification Performance.csv\")\n",
    "\n",
    "# Display Results -- Out of Sample (Holdout) prediction -- Sorted by Accuracy #\n",
    "RESULTS_TABLE.sort_values(\"Accuracy\", ascending = False )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b888ac64",
   "metadata": {},
   "source": [
    "# New try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea606633",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e181b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cc9aff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912a4a55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0199ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13372115",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5cd7813f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Define the set of classifiers and their parameters\n",
    "CLASSIFIERS = [\n",
    "    [\"RUSBoost\", RUSBoostClassifier(random_state=1)],\n",
    "    [\"Random Forest\", RandomForestClassifier(random_state=1)],\n",
    "]\n",
    "\n",
    "\n",
    "# Number of Folds (Splits) for Cross Validation #\n",
    "NUM_OF_SPLITS = 5\n",
    "\n",
    "# Define whether you want to manually reweight the sample by oversampling the smaller class \n",
    "Reweight = False\n",
    "\n",
    "# Define arrays in which to store classification outputs # \n",
    "RESULTS = []\n",
    "Classified_Values =[]\n",
    "Classified_Values_p =[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3e849e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline with the vectorizer and classifier\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(tokenizer=textblob_tokenizer)),\n",
    "    ('classifier', None)\n",
    "])\n",
    "\n",
    "\n",
    "# Define the grid of parameters for the vectorizer and classifiers\n",
    "vectorizer_params = {\n",
    "    'max_features': [200, 2000],\n",
    "    'max_df': [0.7, 0.8],\n",
    "    'ngram_range': [(1, 1), (1, 2)]\n",
    "}\n",
    "\n",
    "classifier_params = {\n",
    "    'RUSBoost': {\n",
    "        'n_estimators': [50, 100],\n",
    "        'learning_rate': [0.1, 0.2]\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [50, 100],\n",
    "        'max_depth': [1, 5, 10]\n",
    "    }\n",
    "}\n",
    "\n",
    "param_grid = {\n",
    "        'vectorizer__max_features': vectorizer_params['max_features'],\n",
    "        'vectorizer__max_df': vectorizer_params['max_df'],\n",
    "        'vectorizer__ngram_range': vectorizer_params['ngram_range'],\n",
    "        'classifier': [CL[1] for CL in CLASSIFIERS],\n",
    "    }\n",
    "\n",
    "\n",
    "# Define Randomized Search #\n",
    "rs = RandomizedSearchCV(pipeline, param_distributions=param_grid, n_iter=10, \n",
    "                            cv=KFoldSplitter, verbose=1, n_jobs= 1, scoring=scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4830897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store Data in Lists for Text Classification #\n",
    "IDs = np.array(df_train['id'].values.tolist())\n",
    "headlines = np.array(df_train['Headline'].values.tolist())\n",
    "Classes = df_train['category'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "413d6c03",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'todense'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-8358aa1cd3b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Headline'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'todense'"
     ]
    }
   ],
   "source": [
    "print(df_train['Headline'][8].todense().tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ec367add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc7d732335bb48e9ac4473cbe9174502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Classifiers:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7efd8876a76640eb860f754df69cbaff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Cross-Validating:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 50 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n30 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/Artur/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/Artur/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 405, in fit\n    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n  File \"/Users/Artur/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\", line 340, in fit\n    self._validate_params()\n  File \"/Users/Artur/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py\", line 600, in _validate_params\n    validate_parameter_constraints(\n  File \"/Users/Artur/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/_param_validation.py\", line 97, in validate_parameter_constraints\n    raise InvalidParameterError(\nsklearn.utils._param_validation.InvalidParameterError: The 'max_depth' parameter of RandomForestClassifier must be an int in the range [1, inf) or None. Got [1, 5, 10] instead.\n\n--------------------------------------------------------------------------------\n20 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/Artur/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/Artur/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 405, in fit\n    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n  File \"/Users/Artur/opt/anaconda3/lib/python3.8/site-packages/imblearn/ensemble/_weight_boosting.py\", line 244, in fit\n    self._validate_params()\n  File \"/Users/Artur/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py\", line 600, in _validate_params\n    validate_parameter_constraints(\n  File \"/Users/Artur/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/_param_validation.py\", line 97, in validate_parameter_constraints\n    raise InvalidParameterError(\nsklearn.utils._param_validation.InvalidParameterError: The 'learning_rate' parameter of RUSBoostClassifier must be a float in the range (0, inf). Got [0.1, 0.2] instead.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-741be4b5c083>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;31m# Train Model #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mResults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_balanced\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_balanced\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;31m# Perform Prediction on Holdout Sample #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    872\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1767\u001b[0m         \u001b[0;34m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1768\u001b[0;31m         evaluate_candidates(\n\u001b[0m\u001b[1;32m   1769\u001b[0m             ParameterSampler(\n\u001b[1;32m   1770\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    849\u001b[0m                     )\n\u001b[1;32m    850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m                 \u001b[0m_warn_or_raise_about_fit_failures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m                 \u001b[0;31m# For callable self.scoring, the return type is only know after\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    365\u001b[0m                 \u001b[0;34mf\"Below are more details about the failures:\\n{fit_errors_summary}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m             )\n\u001b[0;32m--> 367\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_fits_failed_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: \nAll the 50 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n30 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/Artur/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/Artur/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 405, in fit\n    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n  File \"/Users/Artur/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\", line 340, in fit\n    self._validate_params()\n  File \"/Users/Artur/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py\", line 600, in _validate_params\n    validate_parameter_constraints(\n  File \"/Users/Artur/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/_param_validation.py\", line 97, in validate_parameter_constraints\n    raise InvalidParameterError(\nsklearn.utils._param_validation.InvalidParameterError: The 'max_depth' parameter of RandomForestClassifier must be an int in the range [1, inf) or None. Got [1, 5, 10] instead.\n\n--------------------------------------------------------------------------------\n20 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/Artur/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/Artur/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 405, in fit\n    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n  File \"/Users/Artur/opt/anaconda3/lib/python3.8/site-packages/imblearn/ensemble/_weight_boosting.py\", line 244, in fit\n    self._validate_params()\n  File \"/Users/Artur/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py\", line 600, in _validate_params\n    validate_parameter_constraints(\n  File \"/Users/Artur/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/_param_validation.py\", line 97, in validate_parameter_constraints\n    raise InvalidParameterError(\nsklearn.utils._param_validation.InvalidParameterError: The 'learning_rate' parameter of RUSBoostClassifier must be a float in the range (0, inf). Got [0.1, 0.2] instead.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define best model and metric variables\n",
    "best_model = None\n",
    "best_metric = 0.0\n",
    "\n",
    "# Loop Through Different Classifiers #\n",
    "for CL in tqdm_notebook(CLASSIFIERS, desc = \"Evaluating Classifiers\"):\n",
    "\n",
    "    # Extract Classifier Names & Model #\n",
    "    name  = CL[0]\n",
    "    Model = CL[1]\n",
    "    # Combine the vectorizer and classifier parameter grids\n",
    "    \n",
    "    param_grid = {\n",
    "        'vectorizer__max_features': vectorizer_params['max_features'],\n",
    "        'vectorizer__max_df': vectorizer_params['max_df'],\n",
    "        'vectorizer__ngram_range': vectorizer_params['ngram_range'],\n",
    "        'classifier': [\n",
    "            RUSBoostClassifier(random_state=1, **classifier_params['RUSBoost']),\n",
    "            RandomForestClassifier(random_state=1, **classifier_params['Random Forest'])\n",
    "        ],\n",
    "    }\n",
    "\n",
    "\n",
    "    # Define Arrays to store Actual, Predicted and Ids variables (Because we are shuffling them in next step) # \n",
    "    y_actual = []\n",
    "    y_predicted = []\n",
    "    y_predicted_proba = []\n",
    "\n",
    "    id_s = []\n",
    "\n",
    "    # Loop through K Folds and Repeat Cross Validation #\n",
    "    KFoldSplitter = StratifiedKFold(n_splits=NUM_OF_SPLITS, shuffle=True, random_state=1)\n",
    "    \n",
    "    # Define the custom scorer\n",
    "    scorer = make_scorer(geometric_mean_score)\n",
    "\n",
    "    # Define Randomized Search #\n",
    "    rs = RandomizedSearchCV(pipeline, param_distributions=param_grid, n_iter=10, \n",
    "                            cv=KFoldSplitter, verbose=1, n_jobs= 1, scoring=scorer)\n",
    "\n",
    "    # Define variable to store best model for this classifier\n",
    "    best_model_cl = None\n",
    "    \n",
    "    for train_i, test_i in tqdm_notebook(KFoldSplitter.split(headlines, Classes), \n",
    "                                         desc = 'Cross-Validating',\n",
    "                                         leave = False,\n",
    "                                         total = NUM_OF_SPLITS):\n",
    "\n",
    "        # Select Rows in Data Based on Indexes [train_i, test_i]\n",
    "        Y = np.asarray(Classes)\n",
    "\n",
    "        train_X, test_X = df_train['Headline'][train_i], df_train['Headline'][test_i]\n",
    "        train_y, test_y = Y[train_i], Y[test_i]\n",
    "        Train_IDs, Test_IDs = IDs[train_i], IDs[test_i]\n",
    "\n",
    "\n",
    "\n",
    "        if Reweight == True:\n",
    "            \n",
    "            # Use SMOTE and Tomek links to handle class imbalance\n",
    "            smt = SMOTETomek(random_state=1)\n",
    "            X_train_balanced, y_train_balanced = smt.fit_resample(train_X, train_y)\n",
    "            \n",
    "        else:\n",
    "            X_train_balanced, y_train_balanced = train_X, train_y\n",
    "\n",
    "\n",
    "        # Train Model #\n",
    "        Results = rs.fit(X_train_balanced, y_train_balanced)\n",
    "        \n",
    "        # Perform Prediction on Holdout Sample # \n",
    "        y_pred = Model.predict(test_X.toarray())\n",
    "        y_pred_proba = Model.predict_proba(test_X.toarray())\n",
    "\n",
    "        # Add to List with Final Results # \n",
    "        y_actual = y_actual + list(test_y)\n",
    "        y_predicted = y_predicted + list(y_pred)\n",
    "        y_predicted_proba = y_predicted_proba + list(y_pred_proba)\n",
    "        id_s = id_s + list(Test_IDs)\n",
    "\n",
    "    # ---------------------------------------------------------- #\n",
    "    # This runs only after all of the folds have been classified # \n",
    "    # ---------------------------------------------------------- #\n",
    "\n",
    "    # Calculate classification metrics\n",
    "    Accuracy, Precision, Recall, F1, CM, G_mean, MAUC = get_classification_metrics(y_actual, y_predicted, y_predicted_proba)\n",
    "\n",
    "    # Check if this classifier performed better than the previous best\n",
    "    if G_mean > best_metric:\n",
    "        best_metric = G_mean\n",
    "        best_model = rs.best_estimator_\n",
    "        best_model_cl = name\n",
    "\n",
    "    # Add Classification Performance Metrics to List #\n",
    "    RESULTS.append([name, np.round(Accuracy, 3),\n",
    "                          np.round(Precision, 3),\n",
    "                          np.round(Recall, 3),\n",
    "                          np.round(F1, 3),\n",
    "                          G_mean,\n",
    "                          MAUC])\n",
    "\n",
    "    # Add Classification Results to List # \n",
    "    Classified_Values.append(list(zip(len(id_s)*[name],id_s, y_actual, y_predicted, \n",
    "                                       len(id_s)*[G_mean], len(id_s)*[MAUC])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce459cb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8171912",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392438fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5b19dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac38d4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e61bbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5af69ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd023178",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1d7642",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
