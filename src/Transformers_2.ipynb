{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf43c7f3",
   "metadata": {},
   "source": [
    "# D. Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760c6e3d",
   "metadata": {},
   "source": [
    "In this notebook, I build two transformers model to classify the news headlines into the correct categories. This notebook deviates from the previous notebooks as I did not use the resampling technique or the base classifiers. First, I did not use the resampling techniques as they have not proven to increase the performance in the previous notebooks. Next, the textual data serves as input to the transformer models so there is no need to transform the text into numerical representations and train the base classifiers.\n",
    "\n",
    "In order to train the transformer models, I use the package Simple Transformers in Python. More documentation about this package can be found in the link below. This package is an easy-to-use package to implement transformer models (Miric et al., 2022). These models have been proven to shown great performance in previous natural language processing tasks. Therefore, I train two transformer models on this dataset:\n",
    "\n",
    "1. BERT\n",
    "2. RoBERTa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3029f57b",
   "metadata": {},
   "source": [
    "Moreover, note that I use the preprocessed text data as input for these models and not the original raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c616f6",
   "metadata": {},
   "source": [
    "https://simpletransformers.ai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5e5b6d",
   "metadata": {},
   "source": [
    "# 0. Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "705a75fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Packages #\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from scipy.stats import randint\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "# Sklearn Packages #\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, make_scorer\n",
    "\n",
    "# Transform packages\n",
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31a47d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68ac2a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn of warnings, just to avoid pesky messages that might cause confusion here\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d611c36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Headline</th>\n",
       "      <th>category</th>\n",
       "      <th>cleaned_headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>194578</td>\n",
       "      <td>Head Line: US Patent granted to BASF SE (Delaw...</td>\n",
       "      <td>None</td>\n",
       "      <td>head u patent granted se delaware may titled c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>564295</td>\n",
       "      <td>Societe Generale Launches a Next-Generation Ca...</td>\n",
       "      <td>None</td>\n",
       "      <td>societe generale launch nextgeneration card in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>504138</td>\n",
       "      <td>BARCLAYS PLC Form 8.3 - EUTELSAT COMMUNICATION...</td>\n",
       "      <td>None</td>\n",
       "      <td>plc form communication</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>91379</td>\n",
       "      <td>ASML: 4Q Earnings Snapshot</td>\n",
       "      <td>None</td>\n",
       "      <td>4q earnings snapshot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>265750</td>\n",
       "      <td>Form 8.3 - AXA INVESTMENT MANAGERS : Booker Gr...</td>\n",
       "      <td>None</td>\n",
       "      <td>form investment manager group plc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                           Headline category  \\\n",
       "0  194578  Head Line: US Patent granted to BASF SE (Delaw...     None   \n",
       "1  564295  Societe Generale Launches a Next-Generation Ca...     None   \n",
       "2  504138  BARCLAYS PLC Form 8.3 - EUTELSAT COMMUNICATION...     None   \n",
       "3   91379                         ASML: 4Q Earnings Snapshot     None   \n",
       "4  265750  Form 8.3 - AXA INVESTMENT MANAGERS : Booker Gr...     None   \n",
       "\n",
       "                                    cleaned_headline  \n",
       "0  head u patent granted se delaware may titled c...  \n",
       "1  societe generale launch nextgeneration card in...  \n",
       "2                             plc form communication  \n",
       "3                               4q earnings snapshot  \n",
       "4                  form investment manager group plc  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change to Working Directory with Training Data # \n",
    "#os.chdir(\"/Users/Artur/Desktop/thesis_HIR_versie5/coding\")\n",
    "os.chdir(\"/Users/juarel/Desktop/studies artur/thesis_HIR/coding\")\n",
    "\n",
    "# Load the preprocessed data #\n",
    "df_train = pd.read_csv(\"./data/gold_data/train.csv\", header = 0)\n",
    "df_test = pd.read_csv(\"./data/gold_data/test.csv\", header = 0)\n",
    "\n",
    "# inspect the data\n",
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb86cefd",
   "metadata": {},
   "source": [
    "# 1. Define functions and parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccab087",
   "metadata": {},
   "source": [
    "Before we continue, we first define some useful functions and parameters that we use throughout this notebook. These function will be used to train the model and evaluate the model afterwards.\n",
    "\n",
    "1. get_classification_metrics: Create a function that return the classification metrics for each model. The precision, recall and f1 score are all determined using the average value of all classes, without adjusting weights to these classes.\n",
    "\n",
    "2. First, we define several function that return the classification metrics. These are used instead of the previous 'get_classification_metrics()' function due to easier implementation. Moreover, the f1_multiclass fuction is also used as a scoring metric to optimize the training of the transformers for the F1 metric.\n",
    "\n",
    "3. I define the input parameters for the two models. The Simple Transformers package allows for a wide range of parameters. For more information about these parameters, I refer to the link at the top of the notebook. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa3932e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Function that returns classication metrics\n",
    "def get_classification_metrics(y_true, y_pred):\n",
    "    \n",
    "    # Calculate Model Performance Metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='macro')\n",
    "    recall = recall_score(y_true, y_pred, average='macro')\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "faf61e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define functions that return classication metrics\n",
    "def f1_multiclass(labels, preds):\n",
    "    return f1_score(labels, preds, average='macro')\n",
    "\n",
    "def prec_multiclass(labels, preds):\n",
    "    return precision_score(labels, preds, average='macro')\n",
    "\n",
    "def recall_multiclass(labels, preds):\n",
    "    return recall_score(labels, preds, average='macro')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4aec4f",
   "metadata": {},
   "source": [
    "Here, I define the different parameters that serve as input for my models. Here, there are several things I want to highlight:\n",
    "\n",
    "1. One of the most important decisions was not to include an evaluation set while training the model. I did not include an evaluation set as it reduces the limited dataset drastically. Moreover, results showed that not including the evaluation set resulted in better performance of the model on my test set, probably due to the extra training data to train the model.\n",
    "\n",
    "2. I added a regularization term to prevent overfitting. This is especially relevant as I did not include an evaluation set while training the data.\n",
    "\n",
    "3. I used trained the model to optimize the weighted f1 score, just as all the previous models.\n",
    "\n",
    "4. To determine the max_seq_length, I checked the length of all the headlines in the training dataset. This code can be found below definining the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac3c6b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path for storage BERT files as very large files and can not be pushed in github\n",
    "file_path = '/Users/juarel/Desktop/studies artur/thesis_HIR/big files/BERT/'\n",
    "\n",
    "# 3. Define the different parameters\n",
    "train_args = {\n",
    "    'output_dir': f'{file_path}transformers-outputs',\n",
    "    'best_model_dir': f'{file_path}transformers-outputs/best_model', # directory to save best models at check points\n",
    "    'cache_dir': f'{file_path}transformers-cache_dir',\n",
    "    'tensorboard_dir': f'{file_path}transformers-runs',\n",
    "\n",
    "    'max_seq_length': 69,            # maximum number of tokens (= words) per input, only few observations have more than 69 tokens \n",
    "    'do_lower_case': True,           # Set true when using uncased models\n",
    "    'num_train_epochs': 10,           # The number of times the equivalent of a full training set has been processed\n",
    "    'train_batch_size': 32,          # The training batch size\n",
    "    'learning_rate': 4e-5,           # Controls how fast model weights are updated\n",
    "    'save_steps': 1000,\n",
    "    \"save_model_every_epoch\": False,         # Save a model checkpoint at the end of every epoch.\n",
    "    'overwrite_output_dir': True,            # Overwrite existing saved models in same directory\n",
    "    'no_cache': True,                        # No cache features to disk\n",
    "    'use_multiprocessing': True,             # use multiprocessing when converting data into features\n",
    "\n",
    "    'manual_seed': 7,                  # Ensure results can be reproduced\n",
    "    'weight_decay': 0.001,             # Adds L2 penalty (low due to limited dataset)\n",
    "    'metrics': ['f1_multiclass']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ab7318",
   "metadata": {},
   "source": [
    "Define the length of the different headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "009c9357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29057</th>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7029</th>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7244</th>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29504</th>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11858</th>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15788</th>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6946</th>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32251</th>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10383</th>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20039</th>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Headline words\n",
       "29057             112\n",
       "7029               98\n",
       "7244               92\n",
       "29504              91\n",
       "11858              79\n",
       "15788              79\n",
       "6946               76\n",
       "32251              69\n",
       "10383              68\n",
       "20039              68"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize variables to store the lengths\n",
    "max_length = 0\n",
    "length = []\n",
    "\n",
    "# Iterate over each headline in the cleaned_headline column\n",
    "for sentence in df_train['cleaned_headline']:\n",
    "    \n",
    "   # Split the sentence into words\n",
    "   words = sentence.split()\n",
    "    \n",
    "   # Update the maximum length if the current sentence is longer\n",
    "   l = len(words)\n",
    "   length.append(l)\n",
    "\n",
    "# Look at the mean number of words per headline\n",
    "np.mean(length)\n",
    "\n",
    "# Creating the DataFrame\n",
    "df_lengths = pd.DataFrame({'Headline words': length})\n",
    "\n",
    "# Create a DataFrame from the 'length' array\n",
    "# Sort the DataFrame by 'Sentence Length' column in descending order\n",
    "df_sorted = df_lengths.sort_values(by='Headline words', ascending=False)\n",
    "df_sorted.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab6ad650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the independent and dependent variables\n",
    "X_train = df_train['cleaned_headline']\n",
    "X_test = df_test['cleaned_headline']\n",
    "\n",
    "y_train = df_train['category']\n",
    "y_test = df_test['category']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c12a88",
   "metadata": {},
   "source": [
    "# 2. Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5ea76d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define with what vectorizer we build the models with for storage\n",
    "vectorizer = 'Transformer'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858dc013",
   "metadata": {},
   "source": [
    "#### Transform the labels of the categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f121233",
   "metadata": {},
   "source": [
    "By default, the transfomer models expect the labels of the categories to be integer values from 0 up to the number of labels. Therefore, we first need to adjust the training set to create the right input for both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79ed695b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode the labels as integers for the train and test set\n",
    "df_train[\"label_encoded\"] = label_encoder.fit_transform(df_train[\"category\"])\n",
    "df_test[\"label_encoded\"] = label_encoder.transform(df_test[\"category\"])\n",
    "\n",
    "# Get the number of labels to check\n",
    "num_labels = len(label_encoder.classes_)\n",
    "num_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c042ba",
   "metadata": {},
   "source": [
    "Next, the transformer models require a dataframe as input with in one column the text data and in the other column the different labels as integer values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "937736b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe that serves as input for the transformer models\n",
    "df_train_tranf = df_train[['cleaned_headline', 'label_encoded']]\n",
    "df_train_tranf.columns = [\"text\", \"labels\"]\n",
    "df_train_tranf['labels'].nunique()\n",
    "\n",
    "# Transform the test set for evaluation of the models\n",
    "df_test_transf = df_test[['cleaned_headline', 'label_encoded']]\n",
    "df_test_transf.columns = [\"text\", \"labels\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15b018e",
   "metadata": {},
   "source": [
    "## 2.1 BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8480b80d",
   "metadata": {},
   "source": [
    "First, I train the BERT model. I specify that I use the bert-base-uncased model. This means it uses the base architecture and the model does not make any distinction between lower and upper case. As I cleaned the data and put it in lower case, I want to use an uncased model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d51d42",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "607f4832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformer you use\n",
    "FS = 'Bert'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0faf9a51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model_bert = ClassificationModel('bert', 'bert-base-uncased', num_labels=15, args=train_args, \n",
    "                            use_cuda = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7627e6d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e01337b8648c42a9885304775f498ffe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/43246 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1010e7ab07074eb5a262ad3d072e15d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "598409cb35be4cdaaf0d1ba243736c8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 0 of 10:   0%|          | 0/1352 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b69b9ed9632d407dbccd5196d48161b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 1 of 10:   0%|          | 0/1352 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a33105df41cc4a648d42701b709a41fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 2 of 10:   0%|          | 0/1352 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "568dfb8354134d44a49bb33ac7bd33bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 3 of 10:   0%|          | 0/1352 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c4624b888dd4220a300658e49ff8e13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 4 of 10:   0%|          | 0/1352 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3356ec0c9b264fef8b3a8a0c2bbaf17a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 5 of 10:   0%|          | 0/1352 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2427e27ff45494e8b36cc22f33972f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 6 of 10:   0%|          | 0/1352 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3cd418ccc2d42c2bb00b49f5c358842",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 7 of 10:   0%|          | 0/1352 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5885bc397d6840fe9254bfdd7241054c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 8 of 10:   0%|          | 0/1352 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a276d097be944fd997054dffa5d0ae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 9 of 10:   0%|          | 0/1352 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(13520, 0.10275714916605165)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model_bert.train_model(df_train_tranf, f1=f1_multiclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "718a28f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55e629a5b1be4b5d8645bd5b6d51814e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10810 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0c28565f50c4d089e354ab5deb8fdc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/1352 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate the performance of the model on the test set\n",
    "result_bert, outputs_bert, wrong_predictions_bert = model_bert.eval_model(df_test_transf,\n",
    "                                                            f1=f1_multiclass, acc=accuracy_score,\n",
    "                                                            prec=prec_multiclass,\n",
    "                                                            recall=recall_multiclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "264ca510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mcc': 0.6587668933100075,\n",
       " 'f1': 0.5875861863141217,\n",
       " 'acc': 0.9350601295097132,\n",
       " 'prec': 0.5992932422951209,\n",
       " 'recall': 0.5799231552974732,\n",
       " 'eval_loss': 0.5098160408648408}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the results\n",
    "result_bert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e3667f",
   "metadata": {},
   "source": [
    "Here, I provide the code to make predictions on new test observations and to retrieve their predictions. After this step, I again evaluate the performance of the model on these predictions. however, this should give the same results as the evaluation metrics above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2db2d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the input data to a list of strings\n",
    "X_test = [str(i) for i in df_test_transf['text'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8fd3defd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d1ae2463c6642f98bc5a540dd237730",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10810 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3a9c252565b402780446796bce61ced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1352 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions on new test observations\n",
    "predictions_labeled_bert, raw_outputs_bert = model_bert.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed5f1ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform predicted labels back to original form\n",
    "y_pred_bert = label_encoder.inverse_transform(predictions_labeled_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa4699fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9350601295097132,\n",
       " 0.5992932422951209,\n",
       " 0.5799231552974732,\n",
       " 0.5875861863141217)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the performance of the model\n",
    "accuracy, precision, recall, f1 = get_classification_metrics(y_test, y_pred_bert)\n",
    "accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646df938",
   "metadata": {},
   "source": [
    "Store the results and the predictions in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef3d4555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dataframe to store the results\n",
    "df_results_bert = pd.DataFrame()\n",
    "\n",
    "# Add columns for the metrics\n",
    "columns = ['vectorizer', 'FS', 'accuracy', 'precision', 'recall', 'f1']\n",
    "for col in columns:\n",
    "    df_results_bert[col] = 0\n",
    "\n",
    "# add the results to the dataframe\n",
    "df_results_bert['BERT'] = [vectorizer, FS, accuracy, precision, recall, f1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "14db57c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with the predictions\n",
    "predictions_bert = pd.DataFrame({'Predictions': y_pred_bert})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d39c51",
   "metadata": {},
   "source": [
    "#### write away the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d295b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write away results and predictions\n",
    "df_results_bert.to_csv('./Output/Model performance/results_BERT.csv', index = False, header = True)\n",
    "predictions_bert.to_csv(f'./Output/predictions/BERT.csv', index = True, header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e311fbc7",
   "metadata": {},
   "source": [
    "## 2.2 RoBERTa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81743e07",
   "metadata": {},
   "source": [
    "Note that this is exactly the same code as for the BERT model. The same input data, parameters and procedures are used. Only when defining the model, I define the uncased RoBERTa model instead of the BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "08f673cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path for storage RoBERTa as very large files and can not be pushed in github\n",
    "file_path = '/Users/juarel/Desktop/studies artur/thesis_HIR/big files/RoBERTa/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab1c7b5",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7b5eb9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformer you use\n",
    "FS = 'RoBERTa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "508a65dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model_roberta = ClassificationModel('roberta', 'roberta-base', num_labels=15, args=train_args, \n",
    "                            use_cuda = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe77f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c94149bfcd4441abb06f9f06e9eaf1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/43246 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c05ae1e105b24dcea9ba0e70a5d51e7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d89013bfc28c478babf8c5642c4a9214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 0 of 10:   0%|          | 0/1352 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46727a6bf8cb4e53a29994fbbcd8cefd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 1 of 10:   0%|          | 0/1352 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40835fe1073547698f010d47738627d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 2 of 10:   0%|          | 0/1352 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "107024145e4e41f5864975f9f65c9e62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 3 of 10:   0%|          | 0/1352 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dad840e880243f99c4102d4df6e977a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 4 of 10:   0%|          | 0/1352 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "901e09466635457c8235a7633ccbb5a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 5 of 10:   0%|          | 0/1352 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8918d13a142c4ef7b5a9ff0deb8539e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 6 of 10:   0%|          | 0/1352 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee8c559ef1d242fba65061101d704696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 7 of 10:   0%|          | 0/1352 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d25bd1d4d014498b702a148c9262d81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 8 of 10:   0%|          | 0/1352 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model\n",
    "model_roberta.train_model(df_train_tranf, f1=f1_multiclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9018c1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the performance of the model on the test set\n",
    "result_roberta, outputs_roberta, wrong_predictions_roberta = model_roberta.eval_model(df_test_transf,\n",
    "                                                            f1=f1_multiclass, acc=accuracy_score,\n",
    "                                                            prec=prec_multiclass,\n",
    "                                                            recall=recall_multiclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f598e8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the results\n",
    "result_roberta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f69386f",
   "metadata": {},
   "source": [
    "Here, I provide the code to make predictions on new test observations and to retrieve their predictions. After this step, I again evaluate the performance of the model on these predictions. however, this should give the same results as the evaluation metrics above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abee68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the input data to a list of strings\n",
    "X_test = [str(i) for i in df_test_transf['text'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165d5f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on new test observations\n",
    "predictions_labeled_roberta, raw_outputs_roberta = model_roberta.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1aa3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform predicted labels back to original form\n",
    "y_pred_roberta = label_encoder.inverse_transform(predictions_labeled_roberta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539694b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the performance of the model\n",
    "accuracy, precision, recall, f1 = get_classification_metrics(y_test, y_pred_roberta)\n",
    "accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5c1a43",
   "metadata": {},
   "source": [
    "Store the results and the predictions in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cfa19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dataframe to store the results\n",
    "df_results_roberta = pd.DataFrame()\n",
    "\n",
    "# Add columns for the metrics\n",
    "columns = ['vectorizer', 'FS', 'accuracy', 'precision', 'recall', 'f1']\n",
    "for col in columns:\n",
    "    df_results_roberta[col] = 0\n",
    "\n",
    "# add the results to the dataframe\n",
    "df_results_roberta['RoBERTa'] = [vectorizer, FS, accuracy, precision, recall, f1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0d1ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with the predictions\n",
    "predictions_roberta = pd.DataFrame({'Predictions': y_pred_roberta})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e726df61",
   "metadata": {},
   "source": [
    "#### write away the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35ae610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write away results and predictions\n",
    "df_results_roberta.to_csv('./Output/Model performance/results_RoBERTa.csv', index = False, header = True)\n",
    "predictions_roberta.to_csv(f'./Output/predictions/RoBERTa.csv', index = True, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631ce2da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
